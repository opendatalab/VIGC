<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VIGC</title>
  <link rel="icon" type="image/x-icon" href="static/images/opendatalab_logo.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VIGC: Visual Instruction Generation and Correction</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://wangbindl.github.io/" target="_blank">Bin Wang</a><sup>*</sup><sup>1</sup>,
              </span>

              <span class="author-block">
                Fan Wu <sup>*</sup><sup>1</sup>,
              </span>
              <span class="author-block">
                Xiao Han <sup>*</sup><sup>1</sup>,
              </span>
              <span class="author-block">
                Jiahui Peng <sup>*</sup><sup>1</sup>,
              </span>
              <span class="author-block">
                Huaping Zhong <sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://panzhang0212.github.io/" target="_blank">Pan Zhang</a><sup>1</sup>,
              </span> 
              <br>
              <span class="author-block">
                Xiaoyi Dong</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://liweijia.github.io/" target="_blank">Weijia Li</a><sup>3</sup>,
              </span>
              <span class="author-block">
                Wei Li <sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://myownskyw7.github.io/" target="_blank">Jiaqi Wang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://conghui.github.io/" target="_blank">Conghui He</a><sup>&dagger;</sup><sup>1</sup>,
              </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                    <span class="author-block"><sup>1</sup>Shanghai AI Laboratory &nbsp;&nbsp; <sup>2</sup>SenseTime &nbsp;&nbsp; <sup>3</sup>Sun Yat-Sen University &nbsp;&nbsp;</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution &nbsp; &nbsp; &nbsp; <sup>&dagger;</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span>

                 <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Data</span> 
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks.
          <!-- </p>
          <img width="750px" src="static/images/fig0_introduction-v2_00.png" alt="MY ALT TEXT"/>
          <p> -->
            However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code will be made publicly available.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper Instroduction -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><small>Language-only GPT-4</small> &nbsp; Vs  &nbsp;<small>VIGC</small></h2>
        <div class="content has-text-justified">
          <p>
          </p>
          <img width="750px" src="static/images/fig0_introduction-v2_00.png" alt="MY ALT TEXT"/>
          <p>
            <b>Language-only GPT-4</b>: Current high-quality multimodal fine-tuning data is primarily generated based on language-only GPT-4 as illustrated in Figure 1-(a). This approach necessitates costly manual pre-annotation and restricts the design of questions and generated responses to existing annotated information. Consequently, if the question posed is not within this annotated information, GPT-4 is unable to respond. This method also loses the detailed information in the image for answerable questions. 
          </p>
          <p>  
            <b>VIGC</b>: The proposed VIGC, based on existing Visual Language Models (VLMs), guides the model to generate diverse visual-language question-answer pairs on new images through the fine-tuning of initial instruction data. The ability to generate diverse data is derived from the fact that both the visual encoder and the large language model have been fine-tuned on extensive datasets, encompassing rich image understanding and logical language capabilities. However, we found that data generated directly from provided instructions suffer from severe hallucination issues, which is a common problem plaguing large multimodal models. Fortunately, our visual instruction correction module can significantly reduce model hallucination phenomena through iterative updates.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Instruction -->


<div class="columns is-centered has-text-centered">
  <HR align=center style="border:3 double #000000" width="80%" SIZE=5>
</div>



<!-- Result -->
<!-- <section class="section hero is-light"> -->
<section class="hero section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Generated Instructions by VIGC</h2>
        <div class="content has-text-justified">
         
          <div align="center">

            <img src="static/images/LLaVA_demo.jpg" alt=""/>
            <p>
               <b>LLaVA QA-pairs generated by VIGC</b>
            </p>
          </div>

          <div align="center">

            <img src="static/images/OKVQA_demo.jpg" alt=""/>
            <p>
               <b>OKVQA and A-OKVQA QA-pairs generated byVIGC</b>
            </p>
          </div>


          <!-- We trained the VIGC network using two types of visual-language instruction fine-tuning data:
          </p>
          <p>
            1. <b>Llava 150k</b>: It is manually curated and combined with language-only GPT-4 for multimodal models. It includes 150K training samples, subdivided into simple dialogue (57,669 samples), detailed description (23,240 samples), and complex reasoning vision-language data (76,803 samples). This dataset spans various facets of multimodal dialogue, including category recognition, counting, action recognition, and scene recognition. The detailed descriptions demand careful image observation and comprehensive detailing, while the complex reasoning tasks require deep inference and external knowledge integration.
          </p>
          <p>
              2. <b>A-OKVQA</b> & <b>OKVQA</b>: These datasets, necessitating extensive external knowledge, are ideal for assessing the VIGC's capabilities.
          </p>
          <img src="static/images/fig_llava_demo2_00.png" alt="MY ALT TEXT"/>
          <p>
            Generated Instructions Demo based on the proposed VIGC.
          </p>
          <div align="center"><img width="500px" src="static/images/table1.png" alt="MY ALT TEXT"/></div> -->
          <!-- <p>
            Table 1: Comparative Evaluation of VIGC Data Addition vs. Replacement in Model Training on the LLaVA Evaluation
          </p>
          <div align="center"><img width="900px" src="static/images/table2.png" alt="MY ALT TEXT"/></div>
          <p>
            Table 2: Performance of MiniGPT-4+ Models on MMBench and LLaVA-Eval Datasets. MMBench Metric include Logic Reasoning (LR), Attribute Reasoning (AR), Relation Reasoning (RR), Fine-grained Perception at Instance-level (FP-S), Fine-grained Perception at Cross-instance (FP-C), and Coarse Perception (CP).
          </p>
        </div>
        <div class="content has-text-justified is-light">
          <div align="center"><img width="500px" src="static/images/table3.png" alt="MY ALT TEXT"/></div>
          <p>
            Table 3: Relative scores for different settings w.r.t. GPT-4 (language-only) on LLaVA-Eval Dataset. The results for LLaVA-13B are reproduced from (Liu et al. 2023).
          </p>
          <div align="center"><img width="500px" src="static/images/table4.png" alt="MY ALT TEXT"/></div>
          <p>
          Table 4: Results of finetuning MiniGPT-4+ and Instruct-BLIP on OKVQA and A-OKVQA dataset. -->
          <!-- </p> -->
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<!--&lt;!&ndash; Paper poster &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Poster</h2>-->

<!--      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">-->
<!--          </iframe>-->

<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--&lt;!&ndash;End paper poster &ndash;&gt;-->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is partially borrowed from  <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
